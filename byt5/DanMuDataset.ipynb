{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import argparse\n",
    "import glob\n",
    "import os\n",
    "import json\n",
    "import time\n",
    "import logging\n",
    "import random\n",
    "import re\n",
    "from itertools import chain\n",
    "from string import punctuation\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "import pickle\n",
    "\n",
    "data_dir = \"/media/zihao/New Volume1/UMASS/685_e/github/Zihao_branch/data/Danmu_byt5/pkl\"\n",
    "type_path = 'train'\n",
    "\n",
    "token_file_path = os.path.join(data_dir, type_path, 'danmu_token_single_label_'+type_path+'.pkl')\n",
    "dist_file_path = os.path.join(data_dir, type_path, 'danmu_token_single_label_'+type_path+'.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "token_file = open(token_file_path, \"rb\")\n",
    "dist_file = open(dist_file_path, \"rb\")\n",
    "\n",
    "\n",
    "max_len = 512\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "inputs = []\n",
    "targets = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
      "/home/zihao/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/tokenization_utils_base.py:2217: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  FutureWarning,\n",
      "/home/zihao/anaconda3/envs/py37/lib/python3.7/site-packages/transformers/models/t5/tokenization_t5.py:191: UserWarning: This sequence already has </s>. In future versions this behavior may lead to duplicated eos tokens being added.\n",
      "  f\"This sequence already has {self.eos_token}. In future versions this behavior may lead to duplicated eos tokens being added.\"\n"
     ]
    }
   ],
   "source": [
    "tokens = pickle.load(token_file)\n",
    "labels = pickle.load(dist_file)\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"[.;:!\\'?,\\\"()\\[\\]]\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "for ind, token in enumerate(tokens):\n",
    "    text = token\n",
    "\n",
    "    line = text.strip()\n",
    "    line = REPLACE_NO_SPACE.sub(\"\", line) \n",
    "    line = REPLACE_WITH_SPACE.sub(\"\", line)\n",
    "    line = line + ' </s>'\n",
    "\n",
    "    target = labels[ind] + \" </s>\"\n",
    "\n",
    "    # tokenize inputs\n",
    "    tokenized_inputs = tokenizer.batch_encode_plus(\n",
    "        [line], max_length=max_len, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "    )\n",
    "    # tokenize targets\n",
    "    tokenized_targets = tokenizer.batch_encode_plus(\n",
    "        [target], max_length=2, pad_to_max_length=True, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    inputs.append(tokenized_inputs)\n",
    "    targets.append(tokenized_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81fddcebdcddcb9bd173b56375a42ed7b9a2836b109af7b71e91152a3d8ca675"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
