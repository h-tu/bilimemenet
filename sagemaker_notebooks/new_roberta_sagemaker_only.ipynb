{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4912fc6b",
   "metadata": {},
   "source": [
    "# Get Data Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8651a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a70a5b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1hTp8qDF4r2tTk7W6SBnfYyRoNwwSdkap\n",
      "To: /home/ec2-user/SageMaker/danmu_token_main.pkl\n",
      "100%|███████████████████████████████████████| 5.38M/5.38M [00:00<00:00, 196MB/s]\n",
      "/home/ec2-user/anaconda3/envs/python3/lib/python3.6/site-packages/gdown/cli.py:131: FutureWarning: Option `--id` was deprecated in version 4.3.1 and will be removed in 5.0. You don't need to pass it anymore to use a file ID.\n",
      "  category=FutureWarning,\n",
      "Downloading...\n",
      "From: https://drive.google.com/uc?id=1T-yZGWyPC89SCCbm1m0DrRMGyDlUaonl\n",
      "To: /home/ec2-user/SageMaker/danmu_dist_main.pkl\n",
      "100%|███████████████████████████████████████| 56.7M/56.7M [00:00<00:00, 244MB/s]\n"
     ]
    }
   ],
   "source": [
    "#https://drive.google.com/file/d/1hTp8qDF4r2tTk7W6SBnfYyRoNwwSdkap/view?usp=sharing\n",
    "!gdown --id 1hTp8qDF4r2tTk7W6SBnfYyRoNwwSdkap\n",
    "#https://drive.google.com/file/d/1T-yZGWyPC89SCCbm1m0DrRMGyDlUaonl/view?usp=sharing\n",
    "!gdown --id 1T-yZGWyPC89SCCbm1m0DrRMGyDlUaonl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c30b05c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337392\n",
      "337392\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "token_file = open(\"danmu_token_main.pkl\", \"rb\")\n",
    "dist_file = open(\"danmu_dist_main.pkl\", \"rb\")\n",
    "\n",
    "tokens = pickle.load(token_file)\n",
    "dists = pickle.load(dist_file)\n",
    "\n",
    "print(len(tokens))\n",
    "print(len(dists))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2776eade",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "labels = [np.argmax(i) for i in dists]\n",
    "\n",
    "label_distribution = {}\n",
    "for i in labels:\n",
    "    if i in label_distribution:\n",
    "        label_distribution[i] += 1\n",
    "    else:\n",
    "        label_distribution[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "007608ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(label_distribution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "921fde38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove label with no data\n",
    "label_list = list(label_distribution.keys())\n",
    "labels = [label_list.index(i) for i in labels]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e2842be6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "df = pd.DataFrame({\n",
    "    'text': tokens,\n",
    "    'label': labels\n",
    "})\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2)\n",
    "train_df, val_df = train_test_split(train_df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0cfee0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Debugging\n",
    "train_df = train_df[train_df['text'] != 'nan']\n",
    "test_df = test_df[test_df['text'] != 'nan']\n",
    "val_df = val_df[val_df['text'] != 'nan']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "260d5517",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>握手会</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>瞎折腾</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>我用爷打过的</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>我dd我dd</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>混沌与秩序</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26986</th>\n",
       "      <td>负二贷</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26987</th>\n",
       "      <td>就离大谱</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26988</th>\n",
       "      <td>一副骄傲模样</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26989</th>\n",
       "      <td>姐妹们抢啊</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26990</th>\n",
       "      <td>这传言不真啊</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26991 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         text  label\n",
       "0         握手会      2\n",
       "1         瞎折腾      9\n",
       "2      我用爷打过的      0\n",
       "3      我dd我dd      0\n",
       "4       混沌与秩序      2\n",
       "...       ...    ...\n",
       "26986     负二贷      6\n",
       "26987    就离大谱     13\n",
       "26988  一副骄傲模样      4\n",
       "26989   姐妹们抢啊     14\n",
       "26990  这传言不真啊      4\n",
       "\n",
       "[26991 rows x 2 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.reset_index(drop=True)\n",
    "test_df.reset_index(drop=True)\n",
    "val_df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1ff145",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6f345a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#save to csv\n",
    "\n",
    "train_df = train_df.dropna()\n",
    "test_df = test_df.dropna()\n",
    "val_df = val_df.dropna()\n",
    "\n",
    "train_df.to_csv('data_csv/train.csv', index = False)\n",
    "test_df.to_csv('data_csv/test.csv', index = False)\n",
    "val_df.to_csv('data_csv/val.csv', index = False)\n",
    "\n",
    "# train_dict = train_df.to_dict()\n",
    "# test_dict = test_df.to_dict()\n",
    "# val_dict = val_df.to_dict()\n",
    "# train_data_json = {\n",
    "#     'train' : train_dict,\n",
    "#     'test' : test_dict,\n",
    "#     'validation': val_dict\n",
    "# }\n",
    "\n",
    "# import json\n",
    "\n",
    "# with open('data_csv/data.json', 'w') as f:\n",
    "#     json.dump(train_data_json, f)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20b64bd8",
   "metadata": {},
   "source": [
    "# SageMaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c6d36036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "role: arn:aws:iam::635837196364:role/service-role/AmazonSageMaker-ExecutionRole-20220427T210117\n",
      "bucket: sagemaker-us-east-1-635837196364\n",
      "session region: us-east-1\n"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "\n",
    "session = sagemaker.Session()\n",
    "\n",
    "session_bucket = None\n",
    "if session_bucket == None and session is not None:\n",
    "    session_bucket = session.default_bucket()\n",
    "    \n",
    "role = sagemaker.get_execution_role()\n",
    "session = sagemaker.Session(default_bucket = session_bucket)\n",
    "\n",
    "print(f'role: {role}')\n",
    "print(f'bucket: {session_bucket}')\n",
    "print(f'session region: {session.boto_region_name}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f621f4d1",
   "metadata": {},
   "source": [
    "Upload to S3 bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e73e9f57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'s3://sagemaker-us-east-1-635837196364/dataset/danmu_main/val/val.csv'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import botocore\n",
    "from datasets.filesystems import S3FileSystem\n",
    "\n",
    "s3 = S3FileSystem()\n",
    "s3_prefix = 'dataset/danmu_main'\n",
    "\n",
    "#save data to S3\n",
    "train_input_path = f's3://{session.default_bucket()}/{s3_prefix}/train'\n",
    "#train_dataset.save_to_disk(train_input_path, fs = s3)\n",
    "session.upload_data(path='data_csv/train.csv', bucket=session_bucket, key_prefix=s3_prefix+'/train')\n",
    "\n",
    "test_input_path = f's3://{session.default_bucket()}/{s3_prefix}/test'\n",
    "#test_dataset.save_to_disk(test_input_path, fs = s3)\n",
    "session.upload_data(path='data_csv/test.csv', bucket=session_bucket, key_prefix=s3_prefix+'/test')\n",
    "\n",
    "val_input_path = f's3://{session.default_bucket()}/{s3_prefix}/val'\n",
    "session.upload_data(path='data_csv/val.csv', bucket=session_bucket, key_prefix=s3_prefix+'/val')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9b5f8600",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# gets role for executing training job\n",
    "\n",
    "hyperparameters = {\n",
    "    'model_name_or_path':'uer/chinese_roberta_L-12_H-768',\n",
    "    'output_dir':'/opt/ml/model/chinese_roberta',\n",
    "    # add your remaining hyperparameters \n",
    "    # more info here https://github.com/huggingface/transformers/tree/v4.17.0/examples/pytorch/text-classification\n",
    "    'max_seq_length':128,\n",
    "    'per_device_train_batch_size' : 64,\n",
    "    'learning_rate' : 2e-5,\n",
    "    'num_train_epochs': 5,\n",
    "    #train\n",
    "    'do_train': True,\n",
    "    'do_eval' : True,\n",
    "    #data\n",
    "    'train_file': '/opt/ml/input/data/train/train.csv',\n",
    "    'test_file': '/opt/ml/input/data/train/test.csv',\n",
    "    'validation_file': '/opt/ml/input/data/val/val.csv',\n",
    "    #eval\n",
    "    'evaluation_strategy':\"steps\", \n",
    "    'eval_steps' : 2000,\n",
    "    'load_best_model_at_end':True,\n",
    "    'save_steps':2000,\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0344ebba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# git configuration to download our fine-tuning script\n",
    "git_config = {'repo': 'https://github.com/huggingface/transformers.git','branch': 'v4.17.0'}\n",
    "\n",
    "# creates Hugging Face estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "    #entry_point='run_glue_t5.py',\n",
    "    entry_point='run_glue.py',\n",
    "    #source_dir='./examples/pytorch/text-classification',\n",
    "    source_dir = './scripts',\n",
    "    instance_type='ml.g5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=role,\n",
    "    it_config=git_config,\n",
    "    transformers_version='4.17.0',\n",
    "    pytorch_version='1.10.2',\n",
    "    py_version='py38',\n",
    "    hyperparameters = hyperparameters\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b22fd138",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-05-12 00:36:26 Starting - Starting the training job...\n",
      "2022-05-12 00:36:42 Starting - Preparing the instances for trainingProfilerReport-1652315786: InProgress\n",
      ".........\n",
      "2022-05-12 00:38:27 Downloading - Downloading input data\n",
      "2022-05-12 00:38:27 Training - Downloading the training image.....................\n",
      "2022-05-12 00:41:48 Training - Training image download completed. Training in progress.\u001b[34mbash: cannot set terminal process group (-1): Inappropriate ioctl for device\u001b[0m\n",
      "\u001b[34mbash: no job control in this shell\u001b[0m\n",
      "\u001b[34m2022-05-12 00:41:50,649 sagemaker-training-toolkit INFO     Imported framework sagemaker_pytorch_container.training\u001b[0m\n",
      "\u001b[34m2022-05-12 00:41:50,668 sagemaker_pytorch_container.training INFO     Block until all host DNS lookups succeed.\u001b[0m\n",
      "\u001b[34m2022-05-12 00:41:50,676 sagemaker_pytorch_container.training INFO     Invoking user training script.\u001b[0m\n",
      "\u001b[34m2022-05-12 00:41:51,231 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"test\": \"/opt/ml/input/data/test\",\n",
      "        \"train\": \"/opt/ml/input/data/train\",\n",
      "        \"val\": \"/opt/ml/input/data/val\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_pytorch_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {\n",
      "        \"do_eval\": true,\n",
      "        \"do_train\": true,\n",
      "        \"eval_steps\": 2000,\n",
      "        \"evaluation_strategy\": \"steps\",\n",
      "        \"learning_rate\": 2e-05,\n",
      "        \"load_best_model_at_end\": true,\n",
      "        \"max_seq_length\": 128,\n",
      "        \"model_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "        \"num_train_epochs\": 5,\n",
      "        \"output_dir\": \"/opt/ml/model/chinese_roberta\",\n",
      "        \"per_device_train_batch_size\": 64,\n",
      "        \"save_steps\": 2000,\n",
      "        \"test_file\": \"/opt/ml/input/data/train/test.csv\",\n",
      "        \"train_file\": \"/opt/ml/input/data/train/train.csv\",\n",
      "        \"validation_file\": \"/opt/ml/input/data/val/val.csv\"\n",
      "    },\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"test\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        },\n",
      "        \"val\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-pytorch-training-2022-05-12-00-36-24-865\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-us-east-1-635837196364/huggingface-pytorch-training-2022-05-12-00-36-24-865/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"run_glue\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g5.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g5.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"run_glue.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={\"do_eval\":true,\"do_train\":true,\"eval_steps\":2000,\"evaluation_strategy\":\"steps\",\"learning_rate\":2e-05,\"load_best_model_at_end\":true,\"max_seq_length\":128,\"model_name_or_path\":\"uer/chinese_roberta_L-12_H-768\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model/chinese_roberta\",\"per_device_train_batch_size\":64,\"save_steps\":2000,\"test_file\":\"/opt/ml/input/data/train/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/val/val.csv\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=run_glue.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"test\",\"train\",\"val\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=run_glue\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_pytorch_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-us-east-1-635837196364/huggingface-pytorch-training-2022-05-12-00-36-24-865/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"test\":\"/opt/ml/input/data/test\",\"train\":\"/opt/ml/input/data/train\",\"val\":\"/opt/ml/input/data/val\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_pytorch_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{\"do_eval\":true,\"do_train\":true,\"eval_steps\":2000,\"evaluation_strategy\":\"steps\",\"learning_rate\":2e-05,\"load_best_model_at_end\":true,\"max_seq_length\":128,\"model_name_or_path\":\"uer/chinese_roberta_L-12_H-768\",\"num_train_epochs\":5,\"output_dir\":\"/opt/ml/model/chinese_roberta\",\"per_device_train_batch_size\":64,\"save_steps\":2000,\"test_file\":\"/opt/ml/input/data/train/test.csv\",\"train_file\":\"/opt/ml/input/data/train/train.csv\",\"validation_file\":\"/opt/ml/input/data/val/val.csv\"},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"test\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"},\"val\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-pytorch-training-2022-05-12-00-36-24-865\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-us-east-1-635837196364/huggingface-pytorch-training-2022-05-12-00-36-24-865/source/sourcedir.tar.gz\",\"module_name\":\"run_glue\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g5.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g5.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"run_glue.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[\"--do_eval\",\"True\",\"--do_train\",\"True\",\"--eval_steps\",\"2000\",\"--evaluation_strategy\",\"steps\",\"--learning_rate\",\"2e-05\",\"--load_best_model_at_end\",\"True\",\"--max_seq_length\",\"128\",\"--model_name_or_path\",\"uer/chinese_roberta_L-12_H-768\",\"--num_train_epochs\",\"5\",\"--output_dir\",\"/opt/ml/model/chinese_roberta\",\"--per_device_train_batch_size\",\"64\",\"--save_steps\",\"2000\",\"--test_file\",\"/opt/ml/input/data/train/test.csv\",\"--train_file\",\"/opt/ml/input/data/train/train.csv\",\"--validation_file\",\"/opt/ml/input/data/val/val.csv\"]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TEST=/opt/ml/input/data/test\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_VAL=/opt/ml/input/data/val\u001b[0m\n",
      "\u001b[34mSM_HP_DO_EVAL=true\u001b[0m\n",
      "\u001b[34mSM_HP_DO_TRAIN=true\u001b[0m\n",
      "\u001b[34mSM_HP_EVAL_STEPS=2000\u001b[0m\n",
      "\u001b[34mSM_HP_EVALUATION_STRATEGY=steps\u001b[0m\n",
      "\u001b[34mSM_HP_LEARNING_RATE=2e-05\u001b[0m\n",
      "\u001b[34mSM_HP_LOAD_BEST_MODEL_AT_END=true\u001b[0m\n",
      "\u001b[34mSM_HP_MAX_SEQ_LENGTH=128\u001b[0m\n",
      "\u001b[34mSM_HP_MODEL_NAME_OR_PATH=uer/chinese_roberta_L-12_H-768\u001b[0m\n",
      "\u001b[34mSM_HP_NUM_TRAIN_EPOCHS=5\u001b[0m\n",
      "\u001b[34mSM_HP_OUTPUT_DIR=/opt/ml/model/chinese_roberta\u001b[0m\n",
      "\u001b[34mSM_HP_PER_DEVICE_TRAIN_BATCH_SIZE=64\u001b[0m\n",
      "\u001b[34mSM_HP_SAVE_STEPS=2000\u001b[0m\n",
      "\u001b[34mSM_HP_TEST_FILE=/opt/ml/input/data/train/test.csv\u001b[0m\n",
      "\u001b[34mSM_HP_TRAIN_FILE=/opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34mSM_HP_VALIDATION_FILE=/opt/ml/input/data/val/val.csv\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/opt/conda/bin:/opt/conda/lib/python38.zip:/opt/conda/lib/python3.8:/opt/conda/lib/python3.8/lib-dynload:/opt/conda/lib/python3.8/site-packages:/opt/conda/lib/python3.8/site-packages/smdebug-1.0.13b20220304-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument-3.4.2-py3.8.egg:/opt/conda/lib/python3.8/site-packages/pyinstrument_cext-0.2.4-py3.8-linux-x86_64.egg:/opt/conda/lib/python3.8/site-packages/urllib3-1.26.8-py3.8.egg\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/opt/conda/bin/python3.8 run_glue.py --do_eval True --do_train True --eval_steps 2000 --evaluation_strategy steps --learning_rate 2e-05 --load_best_model_at_end True --max_seq_length 128 --model_name_or_path uer/chinese_roberta_L-12_H-768 --num_train_epochs 5 --output_dir /opt/ml/model/chinese_roberta --per_device_train_batch_size 64 --save_steps 2000 --test_file /opt/ml/input/data/train/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/val/val.csv\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - WARNING - __main__ - Process rank: -1, device: cuda:0, n_gpu: 1distributed training: False, 16-bits training: False\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\u001b[0m\n",
      "\u001b[34m_n_gpu=1,\u001b[0m\n",
      "\u001b[34madafactor=False,\u001b[0m\n",
      "\u001b[34madam_beta1=0.9,\u001b[0m\n",
      "\u001b[34madam_beta2=0.999,\u001b[0m\n",
      "\u001b[34madam_epsilon=1e-08,\u001b[0m\n",
      "\u001b[34mbf16=False,\u001b[0m\n",
      "\u001b[34mbf16_full_eval=False,\u001b[0m\n",
      "\u001b[34mdataloader_drop_last=False,\u001b[0m\n",
      "\u001b[34mdataloader_num_workers=0,\u001b[0m\n",
      "\u001b[34mdataloader_pin_memory=True,\u001b[0m\n",
      "\u001b[34mddp_bucket_cap_mb=None,\u001b[0m\n",
      "\u001b[34mddp_find_unused_parameters=None,\u001b[0m\n",
      "\u001b[34mdebug=[],\u001b[0m\n",
      "\u001b[34mdeepspeed=None,\u001b[0m\n",
      "\u001b[34mdisable_tqdm=False,\u001b[0m\n",
      "\u001b[34mdo_eval=True,\u001b[0m\n",
      "\u001b[34mdo_predict=False,\u001b[0m\n",
      "\u001b[34mdo_train=True,\u001b[0m\n",
      "\u001b[34meval_accumulation_steps=None,\u001b[0m\n",
      "\u001b[34meval_steps=2000,\u001b[0m\n",
      "\u001b[34mevaluation_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mfp16=False,\u001b[0m\n",
      "\u001b[34mfp16_backend=auto,\u001b[0m\n",
      "\u001b[34mfp16_full_eval=False,\u001b[0m\n",
      "\u001b[34mfp16_opt_level=O1,\u001b[0m\n",
      "\u001b[34mgradient_accumulation_steps=1,\u001b[0m\n",
      "\u001b[34mgradient_checkpointing=False,\u001b[0m\n",
      "\u001b[34mgreater_is_better=False,\u001b[0m\n",
      "\u001b[34mgroup_by_length=False,\u001b[0m\n",
      "\u001b[34mhalf_precision_backend=auto,\u001b[0m\n",
      "\u001b[34mhub_model_id=None,\u001b[0m\n",
      "\u001b[34mhub_strategy=HubStrategy.EVERY_SAVE,\u001b[0m\n",
      "\u001b[34mhub_token=<HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mignore_data_skip=False,\u001b[0m\n",
      "\u001b[34mlabel_names=None,\u001b[0m\n",
      "\u001b[34mlabel_smoothing_factor=0.0,\u001b[0m\n",
      "\u001b[34mlearning_rate=2e-05,\u001b[0m\n",
      "\u001b[34mlength_column_name=length,\u001b[0m\n",
      "\u001b[34mload_best_model_at_end=True,\u001b[0m\n",
      "\u001b[34mlocal_rank=-1,\u001b[0m\n",
      "\u001b[34mlog_level=-1,\u001b[0m\n",
      "\u001b[34mlog_level_replica=-1,\u001b[0m\n",
      "\u001b[34mlog_on_each_node=True,\u001b[0m\n",
      "\u001b[34mlogging_dir=/opt/ml/model/chinese_roberta/runs/May12_00-41-55_algo-1,\u001b[0m\n",
      "\u001b[34mlogging_first_step=False,\u001b[0m\n",
      "\u001b[34mlogging_nan_inf_filter=True,\u001b[0m\n",
      "\u001b[34mlogging_steps=500,\u001b[0m\n",
      "\u001b[34mlogging_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34mlr_scheduler_type=SchedulerType.LINEAR,\u001b[0m\n",
      "\u001b[34mmax_grad_norm=1.0,\u001b[0m\n",
      "\u001b[34mmax_steps=-1,\u001b[0m\n",
      "\u001b[34mmetric_for_best_model=loss,\u001b[0m\n",
      "\u001b[34mmp_parameters=,\u001b[0m\n",
      "\u001b[34mno_cuda=False,\u001b[0m\n",
      "\u001b[34mnum_train_epochs=5.0,\u001b[0m\n",
      "\u001b[34moptim=OptimizerNames.ADAMW_HF,\u001b[0m\n",
      "\u001b[34moutput_dir=/opt/ml/model/chinese_roberta,\u001b[0m\n",
      "\u001b[34moverwrite_output_dir=False,\u001b[0m\n",
      "\u001b[34mpast_index=-1,\u001b[0m\n",
      "\u001b[34mper_device_eval_batch_size=8,\u001b[0m\n",
      "\u001b[34mper_device_train_batch_size=64,\u001b[0m\n",
      "\u001b[34mprediction_loss_only=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub=False,\u001b[0m\n",
      "\u001b[34mpush_to_hub_model_id=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_organization=None,\u001b[0m\n",
      "\u001b[34mpush_to_hub_token=<PUSH_TO_HUB_TOKEN>,\u001b[0m\n",
      "\u001b[34mremove_unused_columns=True,\u001b[0m\n",
      "\u001b[34mreport_to=[],\u001b[0m\n",
      "\u001b[34mresume_from_checkpoint=None,\u001b[0m\n",
      "\u001b[34mrun_name=/opt/ml/model/chinese_roberta,\u001b[0m\n",
      "\u001b[34msave_on_each_node=False,\u001b[0m\n",
      "\u001b[34msave_steps=2000,\u001b[0m\n",
      "\u001b[34msave_strategy=IntervalStrategy.STEPS,\u001b[0m\n",
      "\u001b[34msave_total_limit=None,\u001b[0m\n",
      "\u001b[34mseed=42,\u001b[0m\n",
      "\u001b[34msharded_ddp=[],\u001b[0m\n",
      "\u001b[34mskip_memory_metrics=True,\u001b[0m\n",
      "\u001b[34mtf32=None,\u001b[0m\n",
      "\u001b[34mtpu_metrics_debug=False,\u001b[0m\n",
      "\u001b[34mtpu_num_cores=None,\u001b[0m\n",
      "\u001b[34muse_legacy_prediction_loop=False,\u001b[0m\n",
      "\u001b[34mwarmup_ratio=0.0,\u001b[0m\n",
      "\u001b[34mwarmup_steps=0,\u001b[0m\n",
      "\u001b[34mweight_decay=0.0,\u001b[0m\n",
      "\u001b[34mxpu_backend=None,\u001b[0m\n",
      "\u001b[34m)\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - __main__ - load a local file for train: /opt/ml/input/data/train/train.csv\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - __main__ - load a local file for validation: /opt/ml/input/data/val/val.csv\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - WARNING - datasets.builder - Using custom data configuration default-0d14f4fa0e048543\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.builder - Generating dataset csv (/root/.cache/huggingface/datasets/csv/default-0d14f4fa0e048543/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519)\u001b[0m\n",
      "\u001b[34mDownloading and preparing dataset csv/default to /root/.cache/huggingface/datasets/csv/default-0d14f4fa0e048543/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519...\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 10394.81it/s]\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.utils.download_manager - Downloading took 0.0 min\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.utils.download_manager - Checksum Computation took 0.0 min\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 2102.94it/s]\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.utils.info_utils - Unable to verify checksums.\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.builder - Generating split train\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.builder - Generating split validation\u001b[0m\n",
      "\u001b[34m05/12/2022 00:41:56 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\u001b[0m\n",
      "\u001b[34mDataset csv downloaded and prepared to /root/.cache/huggingface/datasets/csv/default-0d14f4fa0e048543/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519. Subsequent calls will reuse this data.\u001b[0m\n",
      "\u001b[34m0%|          | 0/2 [00:00<?, ?it/s]\u001b[0m\n",
      "\u001b[34m100%|██████████| 2/2 [00:00<00:00, 256.78it/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,551 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphkncsuiu\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,551 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmphkncsuiu\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/468 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 468/468 [00:00<00:00, 651kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,582 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,582 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json in cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,582 >> creating metadata file for /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,582 >> creating metadata file for /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,583 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,583 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,584 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,584 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\",\n",
      "    \"3\": \"LABEL_3\",\n",
      "    \"4\": \"LABEL_4\",\n",
      "    \"5\": \"LABEL_5\",\n",
      "    \"6\": \"LABEL_6\",\n",
      "    \"7\": \"LABEL_7\",\n",
      "    \"8\": \"LABEL_8\",\n",
      "    \"9\": \"LABEL_9\",\n",
      "    \"10\": \"LABEL_10\",\n",
      "    \"11\": \"LABEL_11\",\n",
      "    \"12\": \"LABEL_12\",\n",
      "    \"13\": \"LABEL_13\",\n",
      "    \"14\": \"LABEL_14\",\n",
      "    \"15\": \"LABEL_15\",\n",
      "    \"16\": \"LABEL_16\",\n",
      "    \"17\": \"LABEL_17\",\n",
      "    \"18\": \"LABEL_18\"\n",
      "  },\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_10\": 10,\n",
      "    \"LABEL_11\": 11,\n",
      "    \"LABEL_12\": 12,\n",
      "    \"LABEL_13\": 13,\n",
      "    \"LABEL_14\": 14,\n",
      "    \"LABEL_15\": 15,\n",
      "    \"LABEL_16\": 16,\n",
      "    \"LABEL_17\": 17,\n",
      "    \"LABEL_18\": 18,\n",
      "    \"LABEL_2\": 2,\n",
      "    \"LABEL_3\": 3,\n",
      "    \"LABEL_4\": 4,\n",
      "    \"LABEL_5\": 5,\n",
      "    \"LABEL_6\": 6,\n",
      "    \"LABEL_7\": 7,\n",
      "    \"LABEL_8\": 8,\n",
      "    \"LABEL_9\": 9\n",
      "  },\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,615 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4lp7nd97\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,615 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp4lp7nd97\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/264 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 264/264 [00:00<00:00, 365kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,644 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,644 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json in cache at /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,644 >> creating metadata file for /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,644 >> creating metadata file for /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,675 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,675 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,676 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,676 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,740 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi9zfkdh5\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,740 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpi9zfkdh5\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/107k [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 107k/107k [00:00<00:00, 51.7MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,772 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,772 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt in cache at /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,773 >> creating metadata file for /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,773 >> creating metadata file for /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,861 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0zihmm52\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:56,861 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmp0zihmm52\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/112 [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 112/112 [00:00<00:00, 131kB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,891 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:41:56,891 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json in cache at /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,891 >> creating metadata file for /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:41:56,891 >> creating metadata file for /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/vocab.txt from cache at /root/.cache/huggingface/transformers/0be013bc55c3a2fd457b91b27bddb810fc250cdd82faf26e3e1a1ccee896a7e3.accd894ff58c6ff7bd4f3072890776c14f4ea34fcc08e79cd88c2d157756dceb\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/added_tokens.json from cache at None\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/special_tokens_map.json from cache at /root/.cache/huggingface/transformers/79f65e354d46fd5886217dd7fe8c70329d047fd2b61e8262f8c191647c1095fd.dd8bd9bfd3664b530ea4e645105f557769387b3da9f79bdb55ed556bdd80611d\u001b[0m\n",
      "\u001b[34m[INFO|tokenization_utils_base.py:1786] 2022-05-12 00:41:56,921 >> loading file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/tokenizer_config.json from cache at /root/.cache/huggingface/transformers/a925345dc1119384750fc4469cae2142ea2531a29053865661d50b08e751ffd4.687752b9857d35e5d69095e1ce9e005030a5996c0fd67687830bb3827270c17e\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,947 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,947 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,948 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,948 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,997 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:648] 2022-05-12 00:41:56,997 >> loading configuration file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/config.json from cache at /root/.cache/huggingface/transformers/0581d850743cad42501488567cceb8dc9ce50d9f05ad632d273b4389b2f52f68.042085124aedc502028136283b7bf9a169a238009bd6c309f049b249216061a2\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,998 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|configuration_utils.py:684] 2022-05-12 00:41:56,998 >> Model config BertConfig {\n",
      "  \"_name_or_path\": \"uer/chinese_roberta_L-12_H-768\",\n",
      "  \"architectures\": [\n",
      "    \"BertForMaskedLM\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"transformers_version\": \"4.17.0\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 21128\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:57,087 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkcrg79f4\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2215] 2022-05-12 00:41:57,087 >> https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin not found in cache or force_download set to True, downloading to /root/.cache/huggingface/transformers/tmpkcrg79f4\u001b[0m\n",
      "\u001b[34mDownloading:   0%|          | 0.00/390M [00:00<?, ?B/s]\u001b[0m\n",
      "\u001b[34mDownloading:   2%|▏         | 6.63M/390M [00:00<00:05, 69.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   3%|▎         | 13.6M/390M [00:00<00:05, 71.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   5%|▌         | 20.4M/390M [00:00<00:05, 70.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   7%|▋         | 28.0M/390M [00:00<00:05, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:   9%|▉         | 35.5M/390M [00:00<00:04, 75.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  11%|█         | 42.7M/390M [00:00<00:04, 74.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  13%|█▎        | 49.8M/390M [00:00<00:04, 74.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  15%|█▍        | 56.9M/390M [00:00<00:04, 74.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  16%|█▋        | 64.3M/390M [00:00<00:04, 75.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  18%|█▊        | 71.4M/390M [00:01<00:04, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  20%|██        | 78.4M/390M [00:01<00:04, 71.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  22%|██▏       | 85.6M/390M [00:01<00:04, 72.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  24%|██▎       | 92.7M/390M [00:01<00:04, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  26%|██▌       | 99.7M/390M [00:01<00:04, 72.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  27%|██▋       | 107M/390M [00:01<00:04, 72.8MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  29%|██▉       | 114M/390M [00:01<00:03, 73.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  31%|███       | 121M/390M [00:01<00:03, 73.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  33%|███▎      | 128M/390M [00:01<00:04, 68.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  35%|███▍      | 135M/390M [00:01<00:03, 70.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  37%|███▋      | 143M/390M [00:02<00:03, 73.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  38%|███▊      | 150M/390M [00:02<00:03, 71.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  40%|████      | 157M/390M [00:02<00:03, 72.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  42%|████▏     | 164M/390M [00:02<00:03, 72.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  44%|████▍     | 171M/390M [00:02<00:03, 73.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  46%|████▌     | 179M/390M [00:02<00:02, 75.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  48%|████▊     | 186M/390M [00:02<00:02, 73.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  50%|████▉     | 194M/390M [00:02<00:02, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  52%|█████▏    | 201M/390M [00:02<00:02, 76.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  53%|█████▎    | 209M/390M [00:02<00:02, 75.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  55%|█████▌    | 216M/390M [00:03<00:02, 76.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  57%|█████▋    | 223M/390M [00:03<00:02, 76.6MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  59%|█████▉    | 231M/390M [00:03<00:02, 78.0MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  61%|██████    | 239M/390M [00:03<00:02, 76.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  63%|██████▎   | 246M/390M [00:03<00:01, 76.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  65%|██████▍   | 253M/390M [00:03<00:01, 75.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  67%|██████▋   | 260M/390M [00:03<00:01, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  69%|██████▊   | 267M/390M [00:03<00:01, 74.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  70%|███████   | 275M/390M [00:03<00:01, 75.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  72%|███████▏  | 282M/390M [00:03<00:01, 75.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  74%|███████▍  | 289M/390M [00:04<00:01, 75.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  76%|███████▌  | 297M/390M [00:04<00:01, 71.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  78%|███████▊  | 304M/390M [00:04<00:01, 72.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  80%|███████▉  | 311M/390M [00:04<00:01, 74.9MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  82%|████████▏ | 319M/390M [00:04<00:00, 75.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  83%|████████▎ | 326M/390M [00:04<00:00, 73.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  85%|████████▌ | 333M/390M [00:04<00:00, 72.7MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  87%|████████▋ | 340M/390M [00:04<00:00, 73.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  89%|████████▉ | 348M/390M [00:04<00:00, 76.1MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  91%|█████████▏| 356M/390M [00:05<00:00, 79.3MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  93%|█████████▎| 364M/390M [00:05<00:00, 79.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  95%|█████████▌| 372M/390M [00:05<00:00, 80.5MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  97%|█████████▋| 380M/390M [00:05<00:00, 82.2MB/s]\u001b[0m\n",
      "\u001b[34mDownloading:  99%|█████████▉| 388M/390M [00:05<00:00, 80.4MB/s]\u001b[0m\n",
      "\u001b[34mDownloading: 100%|██████████| 390M/390M [00:05<00:00, 74.8MB/s]\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:42:02,577 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2219] 2022-05-12 00:42:02,577 >> storing https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin in cache at /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:42:02,577 >> creating metadata file for /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[INFO|file_utils.py:2227] 2022-05-12 00:42:02,577 >> creating metadata file for /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1431] 2022-05-12 00:42:02,578 >> loading weights file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[INFO|modeling_utils.py:1431] 2022-05-12 00:42:02,578 >> loading weights file https://huggingface.co/uer/chinese_roberta_L-12_H-768/resolve/main/pytorch_model.bin from cache at /root/.cache/huggingface/transformers/8150d6a23b0f518134caba5b6141414848a8adbe069f59c18ddbdd9b7498045c.a61653c00cec4d0c4b7281005a15c05b7277d845fb82d1967138f8296cc7622b\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1693] 2022-05-12 00:42:03,565 >> Some weights of the model checkpoint at uer/chinese_roberta_L-12_H-768 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1704] 2022-05-12 00:42:03,566 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at uer/chinese_roberta_L-12_H-768 and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1693] 2022-05-12 00:42:03,565 >> Some weights of the model checkpoint at uer/chinese_roberta_L-12_H-768 were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34m[WARNING|modeling_utils.py:1704] 2022-05-12 00:42:03,566 >> Some weights of BertForSequenceClassification were not initialized from the model checkpoint at uer/chinese_roberta_L-12_H-768 and are newly initialized: ['bert.pooler.dense.bias', 'classifier.weight', 'classifier.bias', 'bert.pooler.dense.weight']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/243 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m05/12/2022 00:42:03 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-0d14f4fa0e048543/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-949c0dfa6e994771.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 1/243 [00:00<00:27,  8.81ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   1%|          | 3/243 [00:00<00:16, 14.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   2%|▏         | 5/243 [00:00<00:18, 12.59ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   3%|▎         | 7/243 [00:00<00:17, 13.43ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   4%|▎         | 9/243 [00:00<00:16, 14.44ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▍         | 11/243 [00:00<00:15, 15.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   5%|▌         | 13/243 [00:00<00:14, 15.53ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   6%|▌         | 15/243 [00:01<00:14, 15.65ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   7%|▋         | 17/243 [00:01<00:16, 13.63ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   8%|▊         | 19/243 [00:01<00:14, 14.97ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|▊         | 21/243 [00:01<00:14, 15.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   9%|▉         | 23/243 [00:01<00:13, 16.66ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  10%|█         | 25/243 [00:01<00:12, 16.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█         | 27/243 [00:01<00:13, 16.28ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  12%|█▏        | 29/243 [00:01<00:15, 13.74ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  13%|█▎        | 31/243 [00:02<00:15, 14.04ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|█▎        | 33/243 [00:02<00:14, 14.40ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  14%|█▍        | 35/243 [00:02<00:14, 14.68ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  15%|█▌        | 37/243 [00:02<00:13, 15.12ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  16%|█▌        | 39/243 [00:02<00:12, 15.74ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  17%|█▋        | 41/243 [00:02<00:14, 14.31ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  18%|█▊        | 43/243 [00:02<00:12, 15.40ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|█▊        | 45/243 [00:02<00:12, 16.44ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|█▉        | 47/243 [00:03<00:11, 17.18ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  20%|██        | 49/243 [00:03<00:10, 17.82ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  21%|██        | 51/243 [00:03<00:10, 18.26ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  22%|██▏       | 53/243 [00:03<00:11, 15.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 55/243 [00:03<00:11, 16.24ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  23%|██▎       | 57/243 [00:03<00:10, 17.12ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  24%|██▍       | 59/243 [00:03<00:10, 17.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  25%|██▌       | 61/243 [00:03<00:10, 18.07ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  26%|██▋       | 64/243 [00:04<00:10, 16.52ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  27%|██▋       | 66/243 [00:04<00:10, 17.13ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  28%|██▊       | 68/243 [00:04<00:09, 17.60ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  29%|██▉       | 70/243 [00:04<00:09, 17.68ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  30%|██▉       | 72/243 [00:04<00:09, 18.01ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  31%|███       | 75/243 [00:04<00:08, 18.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  32%|███▏      | 77/243 [00:04<00:10, 16.04ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|███▎      | 79/243 [00:04<00:09, 16.59ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|███▎      | 81/243 [00:05<00:09, 17.33ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  34%|███▍      | 83/243 [00:05<00:09, 17.45ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  35%|███▍      | 85/243 [00:05<00:08, 17.72ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  36%|███▌      | 87/243 [00:05<00:08, 17.66ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  37%|███▋      | 89/243 [00:05<00:09, 15.67ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  38%|███▊      | 92/243 [00:05<00:08, 17.17ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  39%|███▊      | 94/243 [00:05<00:08, 17.56ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  40%|███▉      | 96/243 [00:05<00:08, 18.13ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  40%|████      | 98/243 [00:06<00:08, 17.96ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  41%|████      | 100/243 [00:06<00:09, 15.59ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  42%|████▏     | 102/243 [00:06<00:08, 16.33ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  43%|████▎     | 104/243 [00:06<00:08, 16.70ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  44%|████▎     | 106/243 [00:06<00:08, 16.95ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  44%|████▍     | 108/243 [00:06<00:07, 17.14ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  45%|████▌     | 110/243 [00:06<00:07, 17.51ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  46%|████▌     | 112/243 [00:06<00:08, 15.45ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  47%|████▋     | 114/243 [00:07<00:07, 16.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  48%|████▊     | 116/243 [00:07<00:07, 17.09ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|████▊     | 118/243 [00:07<00:07, 17.79ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  49%|████▉     | 120/243 [00:07<00:06, 18.06ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  51%|█████     | 123/243 [00:07<00:06, 18.80ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  51%|█████▏    | 125/243 [00:07<00:07, 16.41ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 127/243 [00:07<00:06, 16.93ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  53%|█████▎    | 129/243 [00:07<00:06, 17.42ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  54%|█████▍    | 132/243 [00:08<00:06, 18.31ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  56%|█████▌    | 135/243 [00:08<00:05, 18.80ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  56%|█████▋    | 137/243 [00:08<00:06, 16.08ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  57%|█████▋    | 139/243 [00:08<00:06, 16.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  58%|█████▊    | 141/243 [00:08<00:05, 17.55ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  59%|█████▉    | 143/243 [00:08<00:05, 17.73ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  60%|█████▉    | 145/243 [00:08<00:05, 17.99ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  60%|██████    | 147/243 [00:08<00:05, 18.07ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  61%|██████▏   | 149/243 [00:09<00:05, 15.91ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  63%|██████▎   | 152/243 [00:09<00:05, 17.11ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  64%|██████▍   | 155/243 [00:09<00:04, 18.03ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  65%|██████▍   | 157/243 [00:09<00:04, 18.17ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  65%|██████▌   | 159/243 [00:09<00:04, 18.10ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  66%|██████▋   | 161/243 [00:09<00:05, 15.53ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  67%|██████▋   | 163/243 [00:09<00:04, 16.37ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  68%|██████▊   | 165/243 [00:09<00:04, 17.25ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  69%|██████▊   | 167/243 [00:10<00:04, 17.96ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  70%|██████▉   | 169/243 [00:10<00:04, 17.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  70%|███████   | 171/243 [00:10<00:03, 18.02ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  71%|███████   | 173/243 [00:10<00:04, 15.79ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  72%|███████▏  | 175/243 [00:10<00:04, 16.54ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  73%|███████▎  | 177/243 [00:10<00:03, 16.97ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  74%|███████▎  | 179/243 [00:10<00:03, 17.31ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  74%|███████▍  | 181/243 [00:10<00:03, 17.66ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  75%|███████▌  | 183/243 [00:10<00:03, 17.46ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  76%|███████▌  | 185/243 [00:11<00:03, 15.39ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  77%|███████▋  | 187/243 [00:11<00:03, 15.94ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  78%|███████▊  | 189/243 [00:11<00:03, 16.81ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  79%|███████▊  | 191/243 [00:11<00:03, 17.30ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  80%|███████▉  | 194/243 [00:11<00:02, 18.33ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  81%|████████  | 196/243 [00:11<00:02, 16.13ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  81%|████████▏ | 198/243 [00:11<00:02, 16.54ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  82%|████████▏ | 200/243 [00:12<00:02, 16.98ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  84%|████████▎ | 203/243 [00:12<00:02, 18.08ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▍ | 206/243 [00:12<00:01, 18.80ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  86%|████████▌ | 208/243 [00:12<00:02, 16.79ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  86%|████████▋ | 210/243 [00:12<00:01, 17.28ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  87%|████████▋ | 212/243 [00:12<00:01, 17.65ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  88%|████████▊ | 214/243 [00:12<00:01, 18.22ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  89%|████████▉ | 216/243 [00:12<00:01, 18.50ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  90%|█████████ | 219/243 [00:13<00:01, 19.12ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  91%|█████████ | 221/243 [00:13<00:01, 16.69ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  92%|█████████▏| 223/243 [00:13<00:01, 17.43ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  93%|█████████▎| 225/243 [00:13<00:01, 17.83ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  93%|█████████▎| 227/243 [00:13<00:00, 17.87ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  94%|█████████▍| 229/243 [00:13<00:00, 18.37ba/s]\u001b[0m\n",
      "\n",
      "2022-05-12 00:42:28 Uploading - Uploading generated training model\u001b[34mRunning tokenizer on dataset:  95%|█████████▌| 231/243 [00:13<00:00, 18.38ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  96%|█████████▌| 233/243 [00:13<00:00, 15.64ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  97%|█████████▋| 236/243 [00:14<00:00, 16.67ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  98%|█████████▊| 238/243 [00:14<00:00, 17.29ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  99%|█████████▉| 240/243 [00:14<00:00, 17.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|█████████▉| 242/243 [00:14<00:00, 18.30ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 243/243 [00:14<00:00, 16.85ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   0%|          | 0/27 [00:00<?, ?ba/s]\u001b[0m\n",
      "\u001b[34m05/12/2022 00:42:18 - INFO - datasets.arrow_dataset - Caching processed dataset at /root/.cache/huggingface/datasets/csv/default-0d14f4fa0e048543/0.0.0/433e0ccc46f9880962cc2b12065189766fbb2bee57a221866138fb9203c83519/cache-8491fb45469f02a4.arrow\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:   4%|▎         | 1/27 [00:00<00:02,  8.86ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  11%|█         | 3/27 [00:00<00:01, 14.92ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  19%|█▊        | 5/27 [00:00<00:01, 15.58ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  26%|██▌       | 7/27 [00:00<00:01, 16.90ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  33%|███▎      | 9/27 [00:00<00:01, 17.72ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  44%|████▍     | 12/27 [00:00<00:00, 18.82ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  52%|█████▏    | 14/27 [00:00<00:00, 16.12ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  59%|█████▉    | 16/27 [00:00<00:00, 16.59ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  67%|██████▋   | 18/27 [00:01<00:00, 17.29ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  74%|███████▍  | 20/27 [00:01<00:00, 17.72ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  85%|████████▌ | 23/27 [00:01<00:00, 18.65ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset:  93%|█████████▎| 25/27 [00:01<00:00, 16.16ba/s]\u001b[0m\n",
      "\u001b[34mRunning tokenizer on dataset: 100%|██████████| 27/27 [00:01<00:00, 16.92ba/s]\u001b[0m\n",
      "\u001b[34mTraceback (most recent call last):\n",
      "  File \"run_glue.py\", line 569, in <module>\u001b[0m\n",
      "\u001b[34mmain()\n",
      "  File \"run_glue.py\", line 429, in main\u001b[0m\n",
      "\u001b[34mraise ValueError(\"--do_predict requires a test dataset\")\u001b[0m\n",
      "\u001b[34mValueError: --do_predict requires a test dataset\u001b[0m\n",
      "\u001b[34m2022-05-12 00:42:20,540 sagemaker-training-toolkit ERROR    Reporting training FAILURE\u001b[0m\n",
      "\u001b[34m2022-05-12 00:42:20,540 sagemaker-training-toolkit ERROR    ExecuteUserScriptError:\u001b[0m\n",
      "\u001b[34mExitCode 1\u001b[0m\n",
      "\u001b[34mErrorMessage \"raise ValueError(\"--do_predict requires a test dataset\")\n",
      " ValueError: --do_predict requires a test dataset\"\u001b[0m\n",
      "\u001b[34mCommand \"/opt/conda/bin/python3.8 run_glue.py --do_eval True --do_train True --eval_steps 2000 --evaluation_strategy steps --learning_rate 2e-05 --load_best_model_at_end True --max_seq_length 128 --model_name_or_path uer/chinese_roberta_L-12_H-768 --num_train_epochs 5 --output_dir /opt/ml/model/chinese_roberta --per_device_train_batch_size 64 --save_steps 2000 --test_file /opt/ml/input/data/train/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/val/val.csv\"\u001b[0m\n",
      "\u001b[34m2022-05-12 00:42:20,540 sagemaker-training-toolkit ERROR    Encountered exit_code 1\u001b[0m\n",
      "\n",
      "2022-05-12 00:42:43 Failed - Training job failed\n",
      "ProfilerReport-1652315786: NoIssuesFound\n"
     ]
    },
    {
     "ename": "UnexpectedStatusException",
     "evalue": "Error for Training job huggingface-pytorch-training-2022-05-12-00-36-24-865: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(\"--do_predict requires a test dataset\")\n ValueError: --do_predict requires a test dataset\"\nCommand \"/opt/conda/bin/python3.8 run_glue.py --do_eval True --do_train True --eval_steps 2000 --evaluation_strategy steps --learning_rate 2e-05 --load_best_model_at_end True --max_seq_length 128 --model_name_or_path uer/chinese_roberta_L-12_H-768 --num_train_epochs 5 --output_dir /opt/ml/model/chinese_roberta --per_device_train_batch_size 64 --save_steps 2000 --test_file /opt/ml/input/data/train/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/val/val.csv\", exit code: 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-b16382994147>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# starting the train job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mhuggingface_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrain_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtest_input_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'val'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mval_input_path\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, inputs, wait, logs, job_name, experiment_config)\u001b[0m\n\u001b[1;32m    953\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatest_training_job\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compilation_job_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/estimator.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, logs)\u001b[0m\n\u001b[1;32m   1954\u001b[0m         \u001b[0;31m# If logs are requested, call logs_for_jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1955\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlogs\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"None\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1956\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogs_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1957\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1958\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msagemaker_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait_for_job\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36mlogs_for_job\u001b[0;34m(self, job_name, wait, poll, log_type)\u001b[0m\n\u001b[1;32m   3796\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3797\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3798\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_check_job_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdescription\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"TrainingJobStatus\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3799\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mdot\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3800\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/python3/lib/python3.6/site-packages/sagemaker/session.py\u001b[0m in \u001b[0;36m_check_job_status\u001b[0;34m(self, job, desc, status_key_name)\u001b[0m\n\u001b[1;32m   3337\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3338\u001b[0m                 \u001b[0mallowed_statuses\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"Completed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"Stopped\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3339\u001b[0;31m                 \u001b[0mactual_status\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3340\u001b[0m             )\n\u001b[1;32m   3341\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnexpectedStatusException\u001b[0m: Error for Training job huggingface-pytorch-training-2022-05-12-00-36-24-865: Failed. Reason: AlgorithmError: ExecuteUserScriptError:\nExitCode 1\nErrorMessage \"raise ValueError(\"--do_predict requires a test dataset\")\n ValueError: --do_predict requires a test dataset\"\nCommand \"/opt/conda/bin/python3.8 run_glue.py --do_eval True --do_train True --eval_steps 2000 --evaluation_strategy steps --learning_rate 2e-05 --load_best_model_at_end True --max_seq_length 128 --model_name_or_path uer/chinese_roberta_L-12_H-768 --num_train_epochs 5 --output_dir /opt/ml/model/chinese_roberta --per_device_train_batch_size 64 --save_steps 2000 --test_file /opt/ml/input/data/train/test.csv --train_file /opt/ml/input/data/train/train.csv --validation_file /opt/ml/input/data/val/val.csv\", exit code: 1"
     ]
    }
   ],
   "source": [
    "\n",
    "# starting the train job\n",
    "huggingface_estimator.fit({'train': train_input_path, 'test': test_input_path, 'val':val_input_path})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eaac320",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
