{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81d914a-9b39-4014-a53b-e0d040eca050",
   "metadata": {},
   "source": [
    "# Fine tune Chinese bert with words\n",
    "\n",
    "CS685 Spring 2022 <br />\n",
    "Apr. 24, 2022<br />\n",
    "Hongyu Tu <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a3feb5-601b-4fc2-a88c-d24bee5cfdcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.autograd import Variable\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eed67ca4-72a5-4ac1-935a-2b5cf0d207e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import datasets \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc68d7eb-7e3a-418e-9357-3284ea7f037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446eab-6efb-445e-9915-f3f54a7f0b52",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "88046e54-591d-4408-a285-8f3af12e935b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# a = [len(i) for i in danmu_token]\n",
    "# plt.hist(a, 50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0b32627-76a6-4378-84bd-baf3f9e152a2",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'danmu_token_main.pkl'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/7w/2gcsqchs7m9_7v3d7rr0bls80000gn/T/ipykernel_17407/3035426550.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'danmu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'comment'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'{}_token_main.pkl'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m         \u001b[0mtmp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mtmp_lst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtmp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'danmu_token_main.pkl'"
     ]
    }
   ],
   "source": [
    "tmp_lst = []\n",
    "\n",
    "for i in ['danmu', 'comment']:\n",
    "    with open('{}_token_main.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "    with open('{}_dist_main.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "        \n",
    "danmu_token, danmu_dist, comment_token, comment_dist = tmp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162abc6-23a0-40fc-8a8c-93fba074712b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "97e02f9a-5b9d-4915-9521-67b7a3528b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [np.argmax(i) for i in danmu_dist]\n",
    "\n",
    "tmp = {}\n",
    "for i in y:\n",
    "    if i in tmp:\n",
    "        tmp[i] += 1\n",
    "    else:\n",
    "        tmp[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12a605f-22b3-400b-9ca5-a32a4f8105c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_l = list(tmp.keys())\n",
    "yy = [t_l.index(i) for i in y]\n",
    "split_idx = int(len(yy) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e30fbad2-0383-4152-be39-d936855268e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 303652\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 33740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict({\"train\": datasets.Dataset.from_dict({\"text\": danmu_token[:split_idx], \"label\": yy[:split_idx]}), \\\n",
    "                                \"test\": datasets.Dataset.from_dict({\"text\": danmu_token[split_idx:], \"label\": yy[split_idx:]})})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a37dd6-9249-4817-961c-3432228f56c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ee52ddf-3791-4530-bac6-d4f39add8997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "939c32e6e03246568345fe2389439032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd637ad4967742faa5af1754f31dc393",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "069de575-ccb5-4ff6-8740-73196f807e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(1000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16d369de-1a1b-4108-8b6b-1205461008a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=len(t_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "14948fe7-ceaf-4a15-bd73-5b5c2a0f92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a4ced55f-481f-4f81-8269-8b12ac41cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f2314a76-b5ed-4d57-a53b-1822791ec284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9ed3c8d5-5bc3-4ea2-90ab-c9a94d4f9b10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7b0d2642-7374-4058-b729-109f2b15e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "11718498-f510-4fc0-af6c-fbae56925f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "C:\\Users\\tomtu\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 1000\n",
      "  Num Epochs = 15\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 1875\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='417' max='1875' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [ 417/1875 03:44 < 13:07, 1.85 it/s, Epoch 3.33/15]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.379289</td>\n",
       "      <td>0.287000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.799761</td>\n",
       "      <td>0.216000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.022844</td>\n",
       "      <td>0.215000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 1000\n",
      "  Batch size = 8\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16956/4032920361.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1398\u001b[0m                         \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1399\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1400\u001b[1;33m                     \u001b[0mtr_loss_step\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m                 if (\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\transformers\\trainer.py\u001b[0m in \u001b[0;36mtraining_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2000\u001b[0m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdeepspeed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2001\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2002\u001b[1;33m             \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2003\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2004\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    361\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    362\u001b[0m                 inputs=inputs)\n\u001b[1;32m--> 363\u001b[1;33m         \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    365\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    171\u001b[0m     \u001b[1;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m     \u001b[1;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[0;32m    174\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
