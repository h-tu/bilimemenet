{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a81d914a-9b39-4014-a53b-e0d040eca050",
   "metadata": {},
   "source": [
    "# Fine tune Chinese bert with words\n",
    "\n",
    "CS685 Spring 2022 <br />\n",
    "Apr. 24, 2022<br />\n",
    "Hongyu Tu <br />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8a3feb5-601b-4fc2-a88c-d24bee5cfdcd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# \n",
    "#!pip3 install torch==1.9.0+cu111 torchvision==0.10.0+cu111 torchaudio==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html\n",
    "#!pip install datasets\n",
    "#!pip install transformers==4.17.0\n",
    "#!pip install numpy\n",
    "#!pip install pickle\n",
    "# from tqdm import tqdm\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import torch.nn.functional as F\n",
    "# import matplotlib.pyplot as plt\n",
    "# from torch.autograd import Variable\n",
    "# from sklearn.model_selection import train_test_split\n",
    "# from transformers import AutoTokenizer, AutoModelForMaskedLM\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eed67ca4-72a5-4ac1-935a-2b5cf0d207e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pickle\n",
    "import datasets \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc68d7eb-7e3a-418e-9357-3284ea7f037c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a446eab-6efb-445e-9915-f3f54a7f0b52",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "88046e54-591d-4408-a285-8f3af12e935b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# a = [len(i) for i in danmu_token]\n",
    "# plt.hist(a, 50)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e0b32627-76a6-4378-84bd-baf3f9e152a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp_lst = []\n",
    "\n",
    "for i in ['danmu', 'comment']:\n",
    "    with open('../data/{}_token_main.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "    with open('../data/{}_dist_main.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "        \n",
    "danmu_token, danmu_dist, comment_token, comment_dist = tmp_lst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5162abc6-23a0-40fc-8a8c-93fba074712b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97e02f9a-5b9d-4915-9521-67b7a3528b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = [np.argmax(i) for i in danmu_dist]\n",
    "\n",
    "tmp = {}\n",
    "for i in y:\n",
    "    if i in tmp:\n",
    "        tmp[i] += 1\n",
    "    else:\n",
    "        tmp[i] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c12a605f-22b3-400b-9ca5-a32a4f8105c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_l = list(tmp.keys())\n",
    "yy = [t_l.index(i) for i in y]\n",
    "split_idx = int(len(yy) * 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e30fbad2-0383-4152-be39-d936855268e7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 303652\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 33740\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = datasets.DatasetDict({\"train\": datasets.Dataset.from_dict({\"text\": danmu_token[:split_idx], \"label\": yy[:split_idx]}), \\\n",
    "                                \"test\": datasets.Dataset.from_dict({\"text\": danmu_token[split_idx:], \"label\": yy[split_idx:]})})\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90a37dd6-9249-4817-961c-3432228f56c1",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Model tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee52ddf-3791-4530-bac6-d4f39add8997",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52ea4cfb88a249879687ed8b0f3520e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/304 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fef225486a443c490f6cd756aac8765",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/34 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-chinese\")\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "069de575-ccb5-4ff6-8740-73196f807e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(50000))\n",
    "# small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(5000))\n",
    "\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42)\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "16d369de-1a1b-4108-8b6b-1205461008a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-chinese were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-chinese and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-chinese\", num_labels=len(t_l))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "14948fe7-ceaf-4a15-bd73-5b5c2a0f92ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4ced55f-481f-4f81-8269-8b12ac41cc5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from datasets import load_metric\n",
    "\n",
    "metric = load_metric(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f2314a76-b5ed-4d57-a53b-1822791ec284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9ed3c8d5-5bc3-4ea2-90ab-c9a94d4f9b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer\n",
    "\n",
    "#training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs = 5)\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"steps\", num_train_epochs = 5, \n",
    "                                  per_device_train_batch_size = 23, save_total_limit = 5, \n",
    "                                  eval_steps = 1000,\n",
    "                                  load_best_model_at_end=True,\n",
    "                                  #save_steps=5000,\n",
    "                                 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b0d2642-7374-4058-b729-109f2b15e540",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11718498-f510-4fc0-af6c-fbae56925f2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model from test_trainer/checkpoint-61000).\n",
      "The following columns in the training set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "/opt/conda/envs/hy/lib/python3.7/site-packages/transformers/optimization.py:309: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  FutureWarning,\n",
      "***** Running training *****\n",
      "  Num examples = 303652\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 23\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 23\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 66015\n",
      "  Continuing training from checkpoint, will skip to saved global_step\n",
      "  Continuing training from epoch 4\n",
      "  Continuing training from global step 61000\n",
      "  Will skip the first 4 epochs then the first 8188 batches in the first epoch. If this takes a lot of time, you can add the `--ignore_data_skip` flag to your launch command, but you will resume the training on data already seen by your model.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1744fc815f9c40818ba9cb3518fa5186",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/8188 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='62511' max='66015' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [62511/66015 27:57 < 1:04:56, 0.90 it/s, Epoch 4.73/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>61500</td>\n",
       "      <td>1.115000</td>\n",
       "      <td>1.962610</td>\n",
       "      <td>0.486307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62000</td>\n",
       "      <td>1.137800</td>\n",
       "      <td>1.903067</td>\n",
       "      <td>0.495614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62500</td>\n",
       "      <td>1.110900</td>\n",
       "      <td>1.938471</td>\n",
       "      <td>0.493361</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 33740\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-61500\n",
      "Configuration saved in test_trainer/checkpoint-61500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-61500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_trainer/checkpoint-1000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-1500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-2000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-2500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-3000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-26500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-27000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-27500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-28000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-28500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-29000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-29500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-30000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-30500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-31000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-31500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-32000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-32500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-33000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-33500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-34000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-34500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-35000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-35500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-36000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-36500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-37000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-37500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-38000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-38500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-39000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-39500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-40000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-40500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-41000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-41500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-42000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-42500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-43000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-43500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-44000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-44500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-45000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-45500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-46000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-46500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-47000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-47500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-48000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-48500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-49000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-49500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-50000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-50500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-51000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-51500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-52000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-52500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-53000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-53500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-54000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-54500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-55000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-55500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-56000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-56500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-57000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-57500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-58000] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-58500] due to args.save_total_limit\n",
      "Deleting older checkpoint [test_trainer/checkpoint-59000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 33740\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-62000\n",
      "Configuration saved in test_trainer/checkpoint-62000/config.json\n",
      "Model weights saved in test_trainer/checkpoint-62000/pytorch_model.bin\n",
      "Deleting older checkpoint [test_trainer/checkpoint-59500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `BertForSequenceClassification.forward` and have been ignored: text. If text are not expected by `BertForSequenceClassification.forward`,  you can safely ignore this message.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 33740\n",
      "  Batch size = 8\n",
      "Saving model checkpoint to test_trainer/checkpoint-62500\n",
      "Configuration saved in test_trainer/checkpoint-62500/config.json\n",
      "Model weights saved in test_trainer/checkpoint-62500/pytorch_model.bin\n",
      "Deleting older checkpoint [test_trainer/checkpoint-60000] due to args.save_total_limit\n"
     ]
    }
   ],
   "source": [
    "\n",
    "trainer.train(resume_from_checkpoint=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4141849-b57b-4e65-9383-e6d354558bed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
