{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "import textwrap\n",
    "from tqdm.auto import tqdm\n",
    "from sklearn import metrics\n",
    "\n",
    "import argparse\n",
    "from T5Finetunner import T5FineTuner, LoggingCallback\n",
    "import glob\n",
    "import random\n",
    "import shutil\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer, AdamW, get_linear_schedule_with_warmup\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pytorch_lightning as pl\n",
    "from T5Finetunner import ImdbDataset \n",
    "import pickle\n",
    "\n",
    "data_path = \"/media/zihao/New Volume1/UMASS/685_e/github/Zihao_branch/data/aclImdb_v1/aclImdb\"\n",
    "print(torch.cuda.is_available())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = dict(\n",
    "    data_dir= data_path, # path for data files\n",
    "    output_dir=\"checkpoints\", # path to save the checkpoints\n",
    "    model_name_or_path='t5-base',\n",
    "    tokenizer_name_or_path='t5-base',\n",
    "    max_seq_length=512,\n",
    "    learning_rate=3e-4,\n",
    "    weight_decay=0.0,\n",
    "    adam_epsilon=1e-8,\n",
    "    warmup_steps=0,\n",
    "    train_batch_size=1,\n",
    "    eval_batch_size=1,\n",
    "    num_train_epochs=2,\n",
    "    gradient_accumulation_steps=16,\n",
    "    n_gpu=1,\n",
    "    early_stop_callback=False,\n",
    "    fp_16=False, # if you want to enable 16-bit training then install apex and set this to true\n",
    "    opt_level='O1', # you can find out more on optimisation levels here https://nvidia.github.io/apex/amp.html#opt-levels-and-properties\n",
    "    max_grad_norm=1.0, # if you enable 16-bit training then set this to a sensible value, 0.5 is a good default\n",
    "    seed=42,\n",
    ")\n",
    "args = argparse.Namespace(**args_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "337392\n",
      "337392\n"
     ]
    }
   ],
   "source": [
    "tmp_lst = []\n",
    "\n",
    "for i in ['danmu']:\n",
    "    with open('{}_token_single_label.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "    with open('{}_dist_single_label.pkl'.format(i), 'rb') as f:\n",
    "        tmp = pickle.load(f)\n",
    "        tmp_lst.append(tmp)\n",
    "        \n",
    "danmu_token, danmu_dist = tmp_lst\n",
    "print(len(danmu_token))\n",
    "print(len(danmu_dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "81fddcebdcddcb9bd173b56375a42ed7b9a2836b109af7b71e91152a3d8ca675"
  },
  "kernelspec": {
   "display_name": "Python 3.7.13 ('py37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
